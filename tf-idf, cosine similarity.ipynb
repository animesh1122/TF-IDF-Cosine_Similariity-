{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment - TFIDF & Cosine Similarity\n",
    "\n",
    "\n",
    "Animesh Patel\n",
    "Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a1.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens1 = get_tokens()\n",
    "count1 = Counter(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1672"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens1 = get_tokens()\n",
    "filtered_1 = [w for w in tokens1 if not w in stopwords.words('english')]\n",
    "count1 = Counter(filtered_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens1, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens1:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_1, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a2.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens2 = get_tokens()\n",
    "count2 = Counter(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4223"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1338"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1338"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens2 = get_tokens()\n",
    "filtered_2 = [w for w in tokens2 if not w in stopwords.words('english')]\n",
    "count2 = Counter(filtered_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2537"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1236"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1236"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens2, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens2:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_2, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a3.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens3 = get_tokens()\n",
    "count3 = Counter(tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1905"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens3 = get_tokens()\n",
    "filtered_3 = [w for w in tokens3 if not w in stopwords.words('english')]\n",
    "count3 = Counter(filtered_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1151"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens3, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens3:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_3, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a4.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens4 = get_tokens()\n",
    "count4 = Counter(tokens4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2885"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1033"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1033"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens4 = get_tokens()\n",
    "filtered_4 = [w for w in tokens4 if not w in stopwords.words('english')]\n",
    "count4 = Counter(filtered_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1792"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens4, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens4:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_4, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a5.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens5 = get_tokens()\n",
    "count5 = Counter(tokens5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens5 = get_tokens()\n",
    "filtered_5 = [w for w in tokens5 if not w in stopwords.words('english')]\n",
    "count5 = Counter(filtered_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens5, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens5:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_5, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a6.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens6 = get_tokens()\n",
    "count6 = Counter(tokens6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1542"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens6 = get_tokens()\n",
    "filtered_6 = [w for w in tokens6 if not w in stopwords.words('english')]\n",
    "count6 = Counter(filtered_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens6, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens6:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_6, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a7.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens7 = get_tokens()\n",
    "count7 = Counter(tokens7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1421"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens7 = get_tokens()\n",
    "filtered_7 = [w for w in tokens7 if not w in stopwords.words('english')]\n",
    "count7 = Counter(filtered_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens7, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens7:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_7, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a8.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens8 = get_tokens()\n",
    "count8 = Counter(tokens8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1591"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens8 = get_tokens()\n",
    "filtered_8 = [w for w in tokens8 if not w in stopwords.words('english')]\n",
    "count8 = Counter(filtered_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens8, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens8:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_8, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a9.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens9 = get_tokens()\n",
    "count9 = Counter(tokens9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens9 = get_tokens()\n",
    "filtered_9 = [w for w in tokens9 if not w in stopwords.words('english')]\n",
    "count9 = Counter(filtered_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens9, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens9:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_9, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "    with open('Data_1/a10.txt', 'r',encoding='utf-8') as shakes:\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "        #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens10 = get_tokens()\n",
    "count10 = Counter(tokens10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1247"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens10 = get_tokens()\n",
    "filtered_10 = [w for w in tokens10 if not w in stopwords.words('english')]\n",
    "count10 = Counter(filtered_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "761"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens10, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens10:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered_10, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the TF-IDF features for each document in your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "path = 'Data_1'\n",
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = [] \n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        shakes = open(file_path, 'r',encoding=\"utf8\")\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "       # no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        no_punctuation = text.translate(str.maketrans('','',string.punctuation))\n",
    "        token_dict[file] = no_punctuation\n",
    "        \n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For all 10 Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 380)\t0.0039054358464855115\n",
      "  (0, 1492)\t0.0039054358464855115\n",
      "  (0, 890)\t0.0039054358464855115\n",
      "  (0, 2119)\t0.0039054358464855115\n",
      "  (0, 2898)\t0.010563220155141247\n",
      "  (0, 697)\t0.010563220155141247\n",
      "  (0, 566)\t0.006984705485077477\n",
      "  (0, 1334)\t0.010563220155141247\n",
      "  (0, 1733)\t0.008979702187436498\n",
      "  (0, 251)\t0.008979702187436498\n",
      "  (0, 1774)\t0.015712356618584142\n",
      "  (0, 1450)\t0.008979702187436498\n",
      "  (0, 2230)\t0.015712356618584142\n",
      "  (0, 2938)\t0.015712356618584142\n",
      "  (0, 2660)\t0.010563220155141247\n",
      "  (0, 1034)\t0.007856178309292071\n",
      "  (0, 1541)\t0.0042776636392283015\n",
      "  (0, 856)\t0.006984705485077477\n",
      "  (0, 2533)\t0.008979702187436498\n",
      "  (0, 1038)\t0.0046891423738825745\n",
      "  (0, 2260)\t0.006272660341587323\n",
      "  (0, 1437)\t0.008979702187436498\n",
      "  (0, 2677)\t0.006984705485077477\n",
      "  (0, 2692)\t0.010563220155141247\n",
      "  (0, 2690)\t0.008979702187436498\n",
      "  :\t:\n",
      "  (9, 2446)\t0.004751235131980083\n",
      "  (9, 1622)\t0.008675597505774813\n",
      "  (9, 2772)\t0.004337798752887406\n",
      "  (9, 1446)\t0.008675597505774813\n",
      "  (9, 2301)\t0.004751235131980083\n",
      "  (9, 2512)\t0.005208267845404175\n",
      "  (9, 2115)\t0.021688993764437035\n",
      "  (9, 2785)\t0.01301339625866222\n",
      "  (9, 2600)\t0.24291673016169477\n",
      "  (9, 179)\t0.07374257879908591\n",
      "  (9, 966)\t0.03904018877598666\n",
      "  (9, 2523)\t0.004337798752887406\n",
      "  (9, 1395)\t0.004337798752887406\n",
      "  (9, 389)\t0.01301339625866222\n",
      "  (9, 596)\t0.005208267845404175\n",
      "  (9, 1007)\t0.004337798752887406\n",
      "  (9, 1010)\t0.004751235131980083\n",
      "  (9, 1045)\t0.030364591270211846\n",
      "  (9, 157)\t0.22122773639725773\n",
      "  (9, 1746)\t0.008675597505774813\n",
      "  (9, 1267)\t0.1865253463741585\n",
      "  (9, 1949)\t0.16049855385683406\n",
      "  (9, 67)\t0.26026792517324443\n",
      "  (9, 1325)\t0.09976937131641035\n",
      "  (9, 1819)\t0.004337798752887406\n"
     ]
    }
   ],
   "source": [
    "print(tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 380)\t0.0039054358464855115\n",
      "  (0, 1492)\t0.0039054358464855115\n",
      "  (0, 890)\t0.0039054358464855115\n",
      "  (0, 2119)\t0.0039054358464855115\n",
      "  (0, 2898)\t0.010563220155141247\n",
      "  (0, 697)\t0.010563220155141247\n",
      "  (0, 566)\t0.006984705485077477\n",
      "  (0, 1334)\t0.010563220155141247\n",
      "  (0, 1733)\t0.008979702187436498\n",
      "  (0, 251)\t0.008979702187436498\n",
      "  (0, 1774)\t0.015712356618584142\n",
      "  (0, 1450)\t0.008979702187436498\n",
      "  (0, 2230)\t0.015712356618584142\n",
      "  (0, 2938)\t0.015712356618584142\n",
      "  (0, 2660)\t0.010563220155141247\n",
      "  (0, 1034)\t0.007856178309292071\n",
      "  (0, 1541)\t0.0042776636392283015\n",
      "  (0, 856)\t0.006984705485077477\n",
      "  (0, 2533)\t0.008979702187436498\n",
      "  (0, 1038)\t0.0046891423738825745\n",
      "  (0, 2260)\t0.006272660341587323\n",
      "  (0, 1437)\t0.008979702187436498\n",
      "  (0, 2677)\t0.006984705485077477\n",
      "  (0, 2692)\t0.010563220155141247\n",
      "  (0, 2690)\t0.008979702187436498\n",
      "  :\t:\n",
      "  (0, 1113)\t0.012832990917684906\n",
      "  (0, 1942)\t0.006272660341587323\n",
      "  (0, 2115)\t0.07029784523673921\n",
      "  (0, 2785)\t0.011716307539456535\n",
      "  (0, 2600)\t0.33977291864423953\n",
      "  (0, 179)\t0.07810871692971023\n",
      "  (0, 966)\t0.02343261507891307\n",
      "  (0, 2523)\t0.007810871692971023\n",
      "  (0, 1395)\t0.007810871692971023\n",
      "  (0, 389)\t0.05467610185079716\n",
      "  (0, 596)\t0.0046891423738825745\n",
      "  (0, 1007)\t0.007810871692971023\n",
      "  (0, 1010)\t0.008555327278456603\n",
      "  (0, 1045)\t0.05858153769728267\n",
      "  (0, 157)\t0.25385333002155824\n",
      "  (0, 1746)\t0.007810871692971023\n",
      "  (0, 1267)\t0.24213702248210173\n",
      "  (0, 2434)\t0.014067427121647724\n",
      "  (0, 1949)\t0.20698809986373212\n",
      "  (0, 67)\t0.16402830555239148\n",
      "  (0, 1325)\t0.10544676785510881\n",
      "  (0, 1819)\t0.015621743385942046\n",
      "  (0, 1132)\t0.010563220155141247\n",
      "  (0, 2942)\t0.010563220155141247\n",
      "  (0, 1131)\t0.6549196496187574\n"
     ]
    }
   ],
   "source": [
    "print(tfs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1256)\t0.007474069655516268\n",
      "  (0, 1797)\t0.009104885965106264\n",
      "  (0, 911)\t0.013034208767078736\n",
      "  (0, 527)\t0.015332715259461217\n",
      "  (0, 244)\t0.013034208767078736\n",
      "  (0, 2538)\t0.015332715259461217\n",
      "  (0, 1175)\t0.015332715259461217\n",
      "  (0, 1455)\t0.015332715259461217\n",
      "  (0, 2822)\t0.015332715259461217\n",
      "  (0, 798)\t0.015332715259461217\n",
      "  (0, 57)\t0.013034208767078736\n",
      "  (0, 542)\t0.011403392457488741\n",
      "  (0, 790)\t0.011403392457488741\n",
      "  (0, 2110)\t0.011403392457488741\n",
      "  (0, 1193)\t0.00823103435296309\n",
      "  (0, 1530)\t0.013034208767078736\n",
      "  (0, 40)\t0.015332715259461217\n",
      "  (0, 2080)\t0.007474069655516268\n",
      "  (0, 768)\t0.015332715259461217\n",
      "  (0, 2309)\t0.015332715259461217\n",
      "  (0, 2409)\t0.015332715259461217\n",
      "  (0, 2389)\t0.015332715259461217\n",
      "  (0, 262)\t0.015332715259461217\n",
      "  (0, 1570)\t0.015332715259461217\n",
      "  (0, 1456)\t0.015332715259461217\n",
      "  :\t:\n",
      "  (0, 1446)\t0.03401288712274569\n",
      "  (0, 2301)\t0.012418220503359596\n",
      "  (0, 2512)\t0.006806379472723786\n",
      "  (0, 1113)\t0.006209110251679798\n",
      "  (0, 1942)\t0.036419543860425056\n",
      "  (0, 2115)\t0.05101933068411853\n",
      "  (0, 2785)\t0.017006443561372844\n",
      "  (0, 2600)\t0.3911482019115754\n",
      "  (0, 179)\t0.04535051616366092\n",
      "  (0, 966)\t0.02267525808183046\n",
      "  (0, 2523)\t0.01133762904091523\n",
      "  (0, 1395)\t0.01133762904091523\n",
      "  (0, 389)\t0.0793634032864066\n",
      "  (0, 596)\t0.006806379472723786\n",
      "  (0, 1007)\t0.01133762904091523\n",
      "  (0, 1010)\t0.012418220503359596\n",
      "  (0, 1045)\t0.06235695972503376\n",
      "  (0, 157)\t0.29477835506379596\n",
      "  (0, 1746)\t0.01133762904091523\n",
      "  (0, 1267)\t0.22108376629784696\n",
      "  (0, 2434)\t0.020419138418171355\n",
      "  (0, 1949)\t0.17573325013418606\n",
      "  (0, 67)\t0.18140206465464367\n",
      "  (0, 1325)\t0.10770747588869467\n",
      "  (0, 1819)\t0.005668814520457615\n"
     ]
    }
   ],
   "source": [
    "print(tfs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2303)\t0.005394313664130211\n",
      "  (0, 572)\t0.005394313664130211\n",
      "  (0, 2510)\t0.004585659438891145\n",
      "  (0, 2081)\t0.004011910135284966\n",
      "  (0, 965)\t0.004585659438891145\n",
      "  (0, 2169)\t0.004585659438891145\n",
      "  (0, 103)\t0.004585659438891145\n",
      "  (0, 1830)\t0.005394313664130211\n",
      "  (0, 1874)\t0.004011910135284966\n",
      "  (0, 2522)\t0.004011910135284966\n",
      "  (0, 2676)\t0.005394313664130211\n",
      "  (0, 1654)\t0.005394313664130211\n",
      "  (0, 910)\t0.005394313664130211\n",
      "  (0, 944)\t0.005394313664130211\n",
      "  (0, 1397)\t0.005394313664130211\n",
      "  (0, 479)\t0.005394313664130211\n",
      "  (0, 2925)\t0.005394313664130211\n",
      "  (0, 1388)\t0.005394313664130211\n",
      "  (0, 2941)\t0.005394313664130211\n",
      "  (0, 650)\t0.005394313664130211\n",
      "  (0, 16)\t0.005394313664130211\n",
      "  (0, 235)\t0.005394313664130211\n",
      "  (0, 1696)\t0.005394313664130211\n",
      "  (0, 2206)\t0.005394313664130211\n",
      "  (0, 2021)\t0.005394313664130211\n",
      "  :\t:\n",
      "  (0, 94)\t0.007977546862289754\n",
      "  (0, 2002)\t0.0019943867155724384\n",
      "  (0, 166)\t0.06382037489831803\n",
      "  (0, 2633)\t0.00436894414406601\n",
      "  (0, 1622)\t0.0019943867155724384\n",
      "  (0, 2772)\t0.005983160146717315\n",
      "  (0, 1446)\t0.003988773431144877\n",
      "  (0, 1113)\t0.002184472072033005\n",
      "  (0, 2115)\t0.02393264058686926\n",
      "  (0, 2785)\t0.009971933577862193\n",
      "  (0, 2600)\t0.6222486552586007\n",
      "  (0, 179)\t0.0259270273024417\n",
      "  (0, 966)\t0.027921414018014137\n",
      "  (0, 2523)\t0.0019943867155724384\n",
      "  (0, 1395)\t0.0019943867155724384\n",
      "  (0, 389)\t0.04985966788931096\n",
      "  (0, 1007)\t0.009971933577862193\n",
      "  (0, 1045)\t0.03390457416473145\n",
      "  (0, 157)\t0.2712365933178516\n",
      "  (0, 1746)\t0.003988773431144877\n",
      "  (0, 1267)\t0.2652534331711343\n",
      "  (0, 1949)\t0.17750041768594702\n",
      "  (0, 67)\t0.19943867155724385\n",
      "  (0, 1325)\t0.08575862876961485\n",
      "  (0, 1819)\t0.013960707009007069\n"
     ]
    }
   ],
   "source": [
    "print(tfs[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 43)\t0.008632431941008253\n",
      "  (0, 391)\t0.010154710831582163\n",
      "  (0, 431)\t0.010154710831582163\n",
      "  (0, 2871)\t0.010154710831582163\n",
      "  (0, 2596)\t0.008632431941008253\n",
      "  (0, 1274)\t0.010154710831582163\n",
      "  (0, 2743)\t0.010154710831582163\n",
      "  (0, 2659)\t0.010154710831582163\n",
      "  (0, 704)\t0.010154710831582163\n",
      "  (0, 1324)\t0.010154710831582163\n",
      "  (0, 539)\t0.010154710831582163\n",
      "  (0, 1564)\t0.010154710831582163\n",
      "  (0, 2577)\t0.010154710831582163\n",
      "  (0, 159)\t0.010154710831582163\n",
      "  (0, 654)\t0.010154710831582163\n",
      "  (0, 254)\t0.010154710831582163\n",
      "  (0, 2615)\t0.010154710831582163\n",
      "  (0, 1847)\t0.017264863882016507\n",
      "  (0, 328)\t0.008632431941008253\n",
      "  (0, 1227)\t0.010154710831582163\n",
      "  (0, 1127)\t0.015104715759120197\n",
      "  (0, 412)\t0.015104715759120197\n",
      "  (0, 201)\t0.022657073638680296\n",
      "  (0, 1993)\t0.010154710831582163\n",
      "  (0, 1047)\t0.008632431941008253\n",
      "  :\t:\n",
      "  (0, 2772)\t0.007508803397049975\n",
      "  (0, 1446)\t0.007508803397049975\n",
      "  (0, 2301)\t0.008224468798938135\n",
      "  (0, 2512)\t0.004507800098412279\n",
      "  (0, 1113)\t0.020561171997345334\n",
      "  (0, 2115)\t0.01501760679409995\n",
      "  (0, 2785)\t0.018772008492624937\n",
      "  (0, 2600)\t0.40547538344069867\n",
      "  (0, 179)\t0.06382482887492479\n",
      "  (0, 966)\t0.026280811889674915\n",
      "  (0, 2523)\t0.007508803397049975\n",
      "  (0, 1395)\t0.007508803397049975\n",
      "  (0, 389)\t0.0938600424631247\n",
      "  (0, 596)\t0.004507800098412279\n",
      "  (0, 1007)\t0.018772008492624937\n",
      "  (0, 1010)\t0.012336703198407203\n",
      "  (0, 1045)\t0.06757923057344978\n",
      "  (0, 157)\t0.22150970021297428\n",
      "  (0, 1746)\t0.01501760679409995\n",
      "  (0, 1267)\t0.2064920934188743\n",
      "  (0, 2434)\t0.022539000492061395\n",
      "  (0, 1949)\t0.1877200849262494\n",
      "  (0, 67)\t0.22526410191149926\n",
      "  (0, 1325)\t0.10512324755869966\n",
      "  (0, 1819)\t0.022526410191149926\n"
     ]
    }
   ],
   "source": [
    "print(tfs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2109)\t0.007230890420895183\n",
      "  (0, 1263)\t0.007230890420895183\n",
      "  (0, 1287)\t0.0053778264248195\n",
      "  (0, 1183)\t0.0061469174717544125\n",
      "  (0, 1875)\t0.0053778264248195\n",
      "  (0, 1424)\t0.007230890420895183\n",
      "  (0, 1667)\t0.007230890420895183\n",
      "  (0, 2005)\t0.0061469174717544125\n",
      "  (0, 923)\t0.0053778264248195\n",
      "  (0, 2921)\t0.007230890420895183\n",
      "  (0, 2655)\t0.0061469174717544125\n",
      "  (0, 2148)\t0.007230890420895183\n",
      "  (0, 549)\t0.007230890420895183\n",
      "  (0, 2325)\t0.007230890420895183\n",
      "  (0, 735)\t0.0061469174717544125\n",
      "  (0, 2432)\t0.007230890420895183\n",
      "  (0, 1430)\t0.007230890420895183\n",
      "  (0, 408)\t0.007230890420895183\n",
      "  (0, 625)\t0.0053778264248195\n",
      "  (0, 546)\t0.007230890420895183\n",
      "  (0, 255)\t0.007230890420895183\n",
      "  (0, 1389)\t0.007230890420895183\n",
      "  (0, 22)\t0.007230890420895183\n",
      "  (0, 461)\t0.007230890420895183\n",
      "  (0, 2455)\t0.007230890420895183\n",
      "  :\t:\n",
      "  (0, 1446)\t0.013367031187941355\n",
      "  (0, 2301)\t0.005856418133565626\n",
      "  (0, 2512)\t0.0032098805265379626\n",
      "  (0, 1113)\t0.00878462720034844\n",
      "  (0, 1942)\t0.008587706951357464\n",
      "  (0, 2115)\t0.05079471851417715\n",
      "  (0, 2785)\t0.013367031187941355\n",
      "  (0, 2600)\t0.4892333414786536\n",
      "  (0, 179)\t0.05079471851417715\n",
      "  (0, 966)\t0.016040437425529627\n",
      "  (0, 2523)\t0.0053468124751765425\n",
      "  (0, 1395)\t0.0053468124751765425\n",
      "  (0, 389)\t0.05346812475176542\n",
      "  (0, 596)\t0.006419761053075925\n",
      "  (0, 1007)\t0.010693624950353085\n",
      "  (0, 1010)\t0.005856418133565626\n",
      "  (0, 1045)\t0.04277449980141234\n",
      "  (0, 157)\t0.29942149860988637\n",
      "  (0, 1746)\t0.010693624950353085\n",
      "  (0, 1267)\t0.20050546781912032\n",
      "  (0, 2434)\t0.006419761053075925\n",
      "  (0, 1949)\t0.21654590524464995\n",
      "  (0, 67)\t0.16307778049288454\n",
      "  (0, 1325)\t0.09624262455317775\n",
      "  (0, 1819)\t0.02138724990070617\n"
     ]
    }
   ],
   "source": [
    "print(tfs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1043)\t0.0064611728877928855\n",
      "  (0, 1700)\t0.005492587247240172\n",
      "  (0, 1352)\t0.0064611728877928855\n",
      "  (0, 1374)\t0.0064611728877928855\n",
      "  (0, 124)\t0.0064611728877928855\n",
      "  (0, 1748)\t0.0064611728877928855\n",
      "  (0, 1963)\t0.0064611728877928855\n",
      "  (0, 509)\t0.0064611728877928855\n",
      "  (0, 510)\t0.005492587247240172\n",
      "  (0, 1806)\t0.0064611728877928855\n",
      "  (0, 1750)\t0.0064611728877928855\n",
      "  (0, 1714)\t0.010985174494480344\n",
      "  (0, 2703)\t0.0064611728877928855\n",
      "  (0, 720)\t0.0064611728877928855\n",
      "  (0, 2412)\t0.0064611728877928855\n",
      "  (0, 101)\t0.0064611728877928855\n",
      "  (0, 1576)\t0.0064611728877928855\n",
      "  (0, 524)\t0.012922345775585771\n",
      "  (0, 1853)\t0.0064611728877928855\n",
      "  (0, 1339)\t0.0064611728877928855\n",
      "  (0, 2372)\t0.0064611728877928855\n",
      "  (0, 2273)\t0.0064611728877928855\n",
      "  (0, 1236)\t0.0064611728877928855\n",
      "  (0, 2113)\t0.0064611728877928855\n",
      "  (0, 81)\t0.0064611728877928855\n",
      "  :\t:\n",
      "  (0, 1622)\t0.007166478356709748\n",
      "  (0, 2772)\t0.0023888261189032494\n",
      "  (0, 1446)\t0.016721782832322746\n",
      "  (0, 2301)\t0.005233011131634372\n",
      "  (0, 1113)\t0.007849516697451557\n",
      "  (0, 2115)\t0.021499435070129243\n",
      "  (0, 2785)\t0.014332956713419496\n",
      "  (0, 2600)\t0.46582109318613363\n",
      "  (0, 179)\t0.047776522378064984\n",
      "  (0, 966)\t0.019110608951225995\n",
      "  (0, 2523)\t0.007166478356709748\n",
      "  (0, 1395)\t0.007166478356709748\n",
      "  (0, 389)\t0.08599774028051697\n",
      "  (0, 596)\t0.02294554814017373\n",
      "  (0, 1007)\t0.019110608951225995\n",
      "  (0, 1010)\t0.005233011131634372\n",
      "  (0, 1045)\t0.05494300073477473\n",
      "  (0, 157)\t0.2460490902470347\n",
      "  (0, 1746)\t0.016721782832322746\n",
      "  (0, 1267)\t0.23888261189032495\n",
      "  (0, 2434)\t0.0028681935175217162\n",
      "  (0, 1949)\t0.14094074101529172\n",
      "  (0, 67)\t0.17199548056103395\n",
      "  (0, 1325)\t0.08360891416161373\n",
      "  (0, 1819)\t0.011944130594516246\n"
     ]
    }
   ],
   "source": [
    "print(tfs[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1035)\t0.012610533965867074\n",
      "  (0, 2424)\t0.012610533965867074\n",
      "  (0, 1269)\t0.012610533965867074\n",
      "  (0, 1777)\t0.00937882595990551\n",
      "  (0, 2666)\t0.012610533965867074\n",
      "  (0, 2393)\t0.012610533965867074\n",
      "  (0, 2094)\t0.010720105969099215\n",
      "  (0, 1144)\t0.012610533965867074\n",
      "  (0, 1350)\t0.012610533965867074\n",
      "  (0, 1008)\t0.012610533965867074\n",
      "  (0, 1036)\t0.012610533965867074\n",
      "  (0, 907)\t0.010720105969099215\n",
      "  (0, 1302)\t0.012610533965867074\n",
      "  (0, 678)\t0.012610533965867074\n",
      "  (0, 1736)\t0.010720105969099215\n",
      "  (0, 69)\t0.012610533965867074\n",
      "  (0, 1001)\t0.012610533965867074\n",
      "  (0, 980)\t0.012610533965867074\n",
      "  (0, 1259)\t0.012610533965867074\n",
      "  (0, 2480)\t0.012610533965867074\n",
      "  (0, 1413)\t0.012610533965867074\n",
      "  (0, 601)\t0.010720105969099215\n",
      "  (0, 2741)\t0.012610533965867074\n",
      "  (0, 1086)\t0.012610533965867074\n",
      "  (0, 1085)\t0.012610533965867074\n",
      "  :\t:\n",
      "  (0, 1446)\t0.00932473822760382\n",
      "  (0, 2301)\t0.010213480704705003\n",
      "  (0, 2512)\t0.005597969966369794\n",
      "  (0, 1113)\t0.005106740352352502\n",
      "  (0, 1942)\t0.007488397963137651\n",
      "  (0, 2115)\t0.01864947645520764\n",
      "  (0, 2785)\t0.01864947645520764\n",
      "  (0, 2600)\t0.37765189821795475\n",
      "  (0, 179)\t0.07926027493463247\n",
      "  (0, 966)\t0.03729895291041528\n",
      "  (0, 2523)\t0.00932473822760382\n",
      "  (0, 1395)\t0.00932473822760382\n",
      "  (0, 389)\t0.04196132202421719\n",
      "  (0, 596)\t0.005597969966369794\n",
      "  (0, 1007)\t0.01864947645520764\n",
      "  (0, 1010)\t0.015320221057057506\n",
      "  (0, 1045)\t0.07926027493463247\n",
      "  (0, 157)\t0.2331184556900955\n",
      "  (0, 1746)\t0.023311845569009552\n",
      "  (0, 1267)\t0.25643030125910504\n",
      "  (0, 2434)\t0.03358781979821876\n",
      "  (0, 1949)\t0.20514424100728407\n",
      "  (0, 67)\t0.20048187189348216\n",
      "  (0, 1325)\t0.07459790582083056\n",
      "  (0, 1819)\t0.01398710734140573\n"
     ]
    }
   ],
   "source": [
    "print(tfs[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 906)\t0.012755667334096776\n",
      "  (0, 2897)\t0.012755667334096776\n",
      "  (0, 518)\t0.012755667334096776\n",
      "  (0, 2082)\t0.012755667334096776\n",
      "  (0, 2839)\t0.012755667334096776\n",
      "  (0, 692)\t0.012755667334096776\n",
      "  (0, 99)\t0.012755667334096776\n",
      "  (0, 2188)\t0.012755667334096776\n",
      "  (0, 45)\t0.012755667334096776\n",
      "  (0, 1916)\t0.012755667334096776\n",
      "  (0, 234)\t0.012755667334096776\n",
      "  (0, 15)\t0.012755667334096776\n",
      "  (0, 2661)\t0.012755667334096776\n",
      "  (0, 1726)\t0.012755667334096776\n",
      "  (0, 1704)\t0.012755667334096776\n",
      "  (0, 1728)\t0.012755667334096776\n",
      "  (0, 612)\t0.012755667334096776\n",
      "  (0, 777)\t0.012755667334096776\n",
      "  (0, 2614)\t0.012755667334096776\n",
      "  (0, 1756)\t0.012755667334096776\n",
      "  (0, 1809)\t0.025511334668193553\n",
      "  (0, 2092)\t0.012755667334096776\n",
      "  (0, 31)\t0.012755667334096776\n",
      "  (0, 1197)\t0.010843482591475865\n",
      "  (0, 42)\t0.012755667334096776\n",
      "  :\t:\n",
      "  (0, 2772)\t0.004716027851429336\n",
      "  (0, 1446)\t0.018864111405717345\n",
      "  (0, 2301)\t0.015496539941733813\n",
      "  (0, 2512)\t0.005662396440194536\n",
      "  (0, 1113)\t0.00516551331391127\n",
      "  (0, 1942)\t0.007574581182815445\n",
      "  (0, 2115)\t0.04244425066286402\n",
      "  (0, 2785)\t0.009432055702858672\n",
      "  (0, 2600)\t0.3914303116686349\n",
      "  (0, 179)\t0.056592334217152034\n",
      "  (0, 966)\t0.028296167108576017\n",
      "  (0, 2523)\t0.009432055702858672\n",
      "  (0, 1395)\t0.009432055702858672\n",
      "  (0, 389)\t0.07074041777144004\n",
      "  (0, 1007)\t0.014148083554288008\n",
      "  (0, 1010)\t0.015496539941733813\n",
      "  (0, 1045)\t0.07074041777144004\n",
      "  (0, 157)\t0.25938153182861345\n",
      "  (0, 1746)\t0.028296167108576017\n",
      "  (0, 1267)\t0.2782456432343308\n",
      "  (0, 2434)\t0.028311982200972682\n",
      "  (0, 1949)\t0.19335714190860276\n",
      "  (0, 67)\t0.17920905835431478\n",
      "  (0, 1325)\t0.07074041777144004\n",
      "  (0, 1819)\t0.018864111405717345\n"
     ]
    }
   ],
   "source": [
    "print(tfs[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1813)\t0.011332060395825778\n",
      "  (0, 1476)\t0.011332060395825778\n",
      "  (0, 935)\t0.011332060395825778\n",
      "  (0, 1846)\t0.011332060395825778\n",
      "  (0, 1102)\t0.022664120791651556\n",
      "  (0, 1373)\t0.011332060395825778\n",
      "  (0, 2213)\t0.011332060395825778\n",
      "  (0, 2276)\t0.011332060395825778\n",
      "  (0, 724)\t0.011332060395825778\n",
      "  (0, 1955)\t0.011332060395825778\n",
      "  (0, 1826)\t0.011332060395825778\n",
      "  (0, 253)\t0.011332060395825778\n",
      "  (0, 1185)\t0.011332060395825778\n",
      "  (0, 2053)\t0.011332060395825778\n",
      "  (0, 2392)\t0.011332060395825778\n",
      "  (0, 2315)\t0.011332060395825778\n",
      "  (0, 722)\t0.011332060395825778\n",
      "  (0, 1049)\t0.011332060395825778\n",
      "  (0, 2059)\t0.011332060395825778\n",
      "  (0, 779)\t0.011332060395825778\n",
      "  (0, 360)\t0.011332060395825778\n",
      "  (0, 2840)\t0.011332060395825778\n",
      "  (0, 1553)\t0.011332060395825778\n",
      "  (0, 1795)\t0.011332060395825778\n",
      "  (0, 2070)\t0.011332060395825778\n",
      "  :\t:\n",
      "  (0, 2772)\t0.016758766449776794\n",
      "  (0, 1446)\t0.029327841287109392\n",
      "  (0, 2301)\t0.004589011873350928\n",
      "  (0, 2512)\t0.005030439942085318\n",
      "  (0, 1113)\t0.004589011873350928\n",
      "  (0, 2115)\t0.025138149674665194\n",
      "  (0, 2785)\t0.008379383224888397\n",
      "  (0, 2600)\t0.42315885285686405\n",
      "  (0, 179)\t0.08379383224888397\n",
      "  (0, 966)\t0.020948458062220992\n",
      "  (0, 2523)\t0.012569074837332597\n",
      "  (0, 1395)\t0.008379383224888397\n",
      "  (0, 389)\t0.05027629934933039\n",
      "  (0, 596)\t0.010060879884170636\n",
      "  (0, 1007)\t0.008379383224888397\n",
      "  (0, 1010)\t0.01835604749340371\n",
      "  (0, 1045)\t0.07960414063643978\n",
      "  (0, 157)\t0.2430021135217635\n",
      "  (0, 1746)\t0.016758766449776794\n",
      "  (0, 1267)\t0.25138149674665194\n",
      "  (0, 2434)\t0.015091319826255953\n",
      "  (0, 1949)\t0.15082889804799116\n",
      "  (0, 67)\t0.14663920643554695\n",
      "  (0, 1325)\t0.05027629934933039\n",
      "  (0, 1819)\t0.016758766449776794\n"
     ]
    }
   ],
   "source": [
    "print(tfs[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF For Documnet-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2834)\t0.011732652901386434\n",
      "  (0, 694)\t0.011732652901386434\n",
      "  (0, 515)\t0.011732652901386434\n",
      "  (0, 1276)\t0.011732652901386434\n",
      "  (0, 1735)\t0.011732652901386434\n",
      "  (0, 1900)\t0.011732652901386434\n",
      "  (0, 1639)\t0.011732652901386434\n",
      "  (0, 1440)\t0.011732652901386434\n",
      "  (0, 1565)\t0.011732652901386434\n",
      "  (0, 1134)\t0.011732652901386434\n",
      "  (0, 1596)\t0.023465305802772868\n",
      "  (0, 713)\t0.011732652901386434\n",
      "  (0, 1974)\t0.011732652901386434\n",
      "  (0, 912)\t0.011732652901386434\n",
      "  (0, 1367)\t0.011732652901386434\n",
      "  (0, 1068)\t0.011732652901386434\n",
      "  (0, 2637)\t0.011732652901386434\n",
      "  (0, 1836)\t0.011732652901386434\n",
      "  (0, 1299)\t0.011732652901386434\n",
      "  (0, 1684)\t0.011732652901386434\n",
      "  (0, 1661)\t0.011732652901386434\n",
      "  (0, 920)\t0.011732652901386434\n",
      "  (0, 1933)\t0.011732652901386434\n",
      "  (0, 1931)\t0.011732652901386434\n",
      "  (0, 170)\t0.011732652901386434\n",
      "  :\t:\n",
      "  (0, 2446)\t0.004751235131980083\n",
      "  (0, 1622)\t0.008675597505774813\n",
      "  (0, 2772)\t0.004337798752887406\n",
      "  (0, 1446)\t0.008675597505774813\n",
      "  (0, 2301)\t0.004751235131980083\n",
      "  (0, 2512)\t0.005208267845404175\n",
      "  (0, 2115)\t0.021688993764437035\n",
      "  (0, 2785)\t0.01301339625866222\n",
      "  (0, 2600)\t0.24291673016169477\n",
      "  (0, 179)\t0.07374257879908591\n",
      "  (0, 966)\t0.03904018877598666\n",
      "  (0, 2523)\t0.004337798752887406\n",
      "  (0, 1395)\t0.004337798752887406\n",
      "  (0, 389)\t0.01301339625866222\n",
      "  (0, 596)\t0.005208267845404175\n",
      "  (0, 1007)\t0.004337798752887406\n",
      "  (0, 1010)\t0.004751235131980083\n",
      "  (0, 1045)\t0.030364591270211846\n",
      "  (0, 157)\t0.22122773639725773\n",
      "  (0, 1746)\t0.008675597505774813\n",
      "  (0, 1267)\t0.1865253463741585\n",
      "  (0, 1949)\t0.16049855385683406\n",
      "  (0, 67)\t0.26026792517324443\n",
      "  (0, 1325)\t0.09976937131641035\n",
      "  (0, 1819)\t0.004337798752887406\n"
     ]
    }
   ],
   "source": [
    "print(tfs[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = open('Data_1/a1.txt','r',encoding='UTF-8')\n",
    "a1 = a1.read()\n",
    "a2 = open('Data_1/a2.txt','r',encoding='UTF-8')\n",
    "a2 = a2.read()\n",
    "a3 = open('Data_1/a3.txt','r',encoding='UTF-8')\n",
    "a3 = a3.read()\n",
    "a4 = open('Data_1/a4.txt','r',encoding='UTF-8')\n",
    "a4 = a4.read()\n",
    "a5 = open('Data_1/a5.txt','r',encoding='UTF-8')\n",
    "a5 = a5.read()\n",
    "a6 = open('Data_1/a6.txt','r',encoding='UTF-8')\n",
    "a6 = a6.read()\n",
    "a7 = open('Data_1/a7.txt','r',encoding='UTF-8')\n",
    "a7 = a7.read()\n",
    "a8 = open('Data_1/a8.txt','r',encoding='UTF-8')\n",
    "a8 = a8.read()\n",
    "a9 = open('Data_1/a9.txt','r',encoding='UTF-8')\n",
    "a9 = a9.read()\n",
    "a10 = open('Data_1/a10.txt','r',encoding='UTF-8')\n",
    "a10 = a10.read()\n",
    "\n",
    "\n",
    "corpus=[a1,a2,a3,a4,a5,a6,a7,a8,a9,a10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.79174732 0.82511546 0.81540283 0.79726513 0.80249743\n",
      "  0.8339161  0.80049095 0.77313788 0.83199773]\n",
      " [0.79174732 1.         0.8171512  0.87511689 0.83766769 0.8214931\n",
      "  0.82760898 0.83120336 0.73365147 0.81223593]\n",
      " [0.82511546 0.8171512  1.         0.82077476 0.82701688 0.80973654\n",
      "  0.84025734 0.8194415  0.77971161 0.82956067]\n",
      " [0.81540283 0.87511689 0.82077476 1.         0.82211779 0.8021618\n",
      "  0.82978506 0.81215394 0.7479282  0.81867287]\n",
      " [0.79726513 0.83766769 0.82701688 0.82211779 1.         0.80684904\n",
      "  0.82894415 0.81781232 0.74859062 0.8096391 ]\n",
      " [0.80249743 0.8214931  0.80973654 0.8021618  0.80684904 1.\n",
      "  0.81945984 0.80811688 0.75647323 0.80160962]\n",
      " [0.8339161  0.82760898 0.84025734 0.82978506 0.82894415 0.81945984\n",
      "  1.         0.83275385 0.77780683 0.83359185]\n",
      " [0.80049095 0.83120336 0.8194415  0.81215394 0.81781232 0.80811688\n",
      "  0.83275385 1.         0.74517654 0.80268935]\n",
      " [0.77313788 0.73365147 0.77971161 0.7479282  0.74859062 0.75647323\n",
      "  0.77780683 0.74517654 1.         0.77106104]\n",
      " [0.83199773 0.81223593 0.82956067 0.81867287 0.8096391  0.80160962\n",
      "  0.83359185 0.80268935 0.77106104 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
